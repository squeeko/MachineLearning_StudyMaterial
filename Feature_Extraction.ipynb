{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many machine learning problems require learning from categorical variables, text or images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from categorical variables\n",
    "\n",
    "### Many problems have explanatory variables that are categorical or nominal. A categorical variable can take one of a fixed set of values. For example, an application that predicts the salary for a job might use categorical variables such as the city in which the position is located. Categorical variables are commonly encoded using 'ONE-OF-K ENCODING' or 'ONE-HOT ENCODING', in which the explanatory variable is represented using one binary feature for each of its possible values.\n",
    "\n",
    "### For example, let's assume our model has a 'city' variable that can take one of three values: New York, San Francisco or Chapel Hill. One-hot encoding represents the variable using one binary feature for each of the three possible cities. scikit-learn's 'DictVectorizer' class is a transformer that can be used to one-hot encode categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "onehot_encoder = DictVectorizer()\n",
    "X = [\n",
    "    {'city': 'New York'},\n",
    "    {'city': 'San Francisco'},\n",
    "    {'city': 'Chapel Hill'}\n",
    "]\n",
    "\n",
    "print(onehot_encoder.fit_transform(X).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that the order of the features in the resulting vectors is arbitrary. In the first training example, the value of 'city' is 'New York'. The second element in the feature vector corresponds to the 'New York' value, and it is equal to one for the first instance. It may seem intuitive to represent the values of a categorical explanatory variable with a single integer feature. 'New York' could be represented by zero, 'San Francisco' by one, and 'Chapel Hill' by two, for example. The problem is that this representation encodes artificial information. Representing cities with integers encodes an order for cities that does not exist in the real world, and facilitates comparisons of them that do not make sense. There is no natural order of cities by which 'Chapel Hill' is one more than 'San Francisco'. One-hot encoding avoids this problem and only represents the value of the variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing Features\n",
    "### We learned in the previous chapter that many learning algorithms perform better when they are trained on standardized data. Recall that standardized data has zero mean and unit variance. An explanatory variable with zero mean is centered about the origin; its average value is zero. A feature vector has unit variance when the variances of its features are all of the same order of magnitude. If one feature's variance is orders of magnitude greater than the variances of the other features, that feature may dominate the learning algorithm and prevent it from learning from the other variables. Some learning algorithms also converge to the optimal parameter values more slowly when data is not standardized. In addition to the 'StandardScaler' transformer we used in the previous chapter, the 'scale' function from the 'preprocessing' module can be used to standardize a dataset along any axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.70710678 -1.38873015  0.52489066  0.59299945 -1.35873244]\n",
      " [ 0.         -0.70710678  0.46291005  0.87481777  0.81537425  1.01904933]\n",
      " [ 0.          1.41421356  0.9258201  -1.39970842 -1.4083737   0.33968311]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X = np.array([\n",
    "    [0., 0., 5., 13., 9., 1.],\n",
    "    [0., 0., 13., 15., 10., 15.],\n",
    "    [0., 3., 15., 2., 0., 11.]\n",
    "])\n",
    "\n",
    "print(preprocessing.scale(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, 'RobustScaler' is an alternative to 'StandardScaler' that is robust to outliers. 'StandardScaler' subtracts the mean of a feature from each instance's value, and divides by the feature's standard deviation. To mitigate the effect of large outliers, 'RobustScaler' subtracts the median and divides by the interquartile range. QUARTILES are calculated by splitting the sorted dataset into four parts of equal size. The MEDIAN is the SECOND QUARTILE; the IQR (INTER QUARTILE RANGE) is the difference of the THIRD and FIRST QUARTILES."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Features From Text\n",
    "### The most common representation of text is the BAG-OF-WORDS model. This representation uses a multiset, or bag, that encodes the words that appear in a text; bag-of-words does not encode any of the text's syntax, ignores the order of words, and disregards all grammar. Bag-of-words can be thought of as an extension to one-hot encoding. It creates one feature for each word of interest in the text. The bag-of-words model is motivated by the intuition that documents containing similar words often have similar meanings. The bag-of-words model can be used effectively for document classification and retrieval despite the limited information that it encodes. A collection of documents is called a CORPUS. \n",
    "\n",
    "### I will create a CORPUS that contains eight unique words. The corpus's unique words comprise its vocabulary. The bag-of-words model uses a feature vector with an element for each of the words in the corpus's vocabulary to represent each document. Our corpus has eight unique words, so each document will be represented by a vector with eight elements. The number of elements that comprise a feature vector is called the vector's dimension. A dictionary maps the vocabulary to indices in the feature vector.\n",
    "\n",
    "### NOTE: The dictionary for a bag-of-words could be implemented using a Python 'Dictionary', but the Python data structure and the representation's mapping are distinct.\n",
    "\n",
    "### In the most basic bag-of-words representation, each element in the feature vector is a binary value that represents whether or not the corresponding word appeared in the document. For example, the first word in the first document is UNC. UNC is the first word in the dictionary, so the first element in the vector is equal to one. The last word in the dictionary is game. The first document does not contain the word game, so the eighth element in its vector is set to zero. The 'CountVectorizer' transformer can produce a bag-of-words representation from a string or file. By default, CountVectorizer' converts the characters in the documents to lowercase and tokenizes the documents. Tokenization is the process of splitting a string into tokens, or meaningful sequences of characters. Tokens are often words, but they may also be shorter sequences, including punctuation characters and affixes. 'CountVectorizer' tokenizes using a regular expression that splits strings on whitespace and extracts sequences of characters that are two or more characters in length. The documents in our corpus are represented by the following feature vectors:\n",
    "\n",
    "### NOTE: CountVectorizer. 'vocabulary_' returns a Dictionary(Key/Value)\n",
    "\n",
    "### 'vocabulary_ : dict'\n",
    "### A mapping of terms to feature indices.\n",
    "\n",
    "### vocabulary : Mapping or iterable, optional' \n",
    "### Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents. Indices in the mapping should not be repeated and should not have any gap between 0 and the largest index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 1 0 1 0]]\n",
      "{'unc': 7, 'played': 5, 'duke': 1, 'in': 3, 'basketball': 0, 'lost': 4, 'the': 6, 'game': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['UNC played Duke in basketball',\n",
    "         'Duke lost the basketball game']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our corpus's dictionary now contains the following ten unique words. Note that 'I' and 'a' were not extracted as they do not match the regular expression. Now let's add a third document to our corpus and inspect the dictionary and feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 0 1 0 0 1]\n",
      " [0 1 1 1 0 1 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 1 0 0]]\n",
      "{'unc': 9, 'played': 6, 'duke': 2, 'in': 4, 'basketball': 1, 'lost': 5, 'the': 8, 'game': 3, 'ate': 0, 'sandwich': 7}\n"
     ]
    }
   ],
   "source": [
    "corpus.append('I ate a sandwich')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The meanings of the first two documents are more similar to each other than they are to the third document, and their corresponding feature vectors are more similar to each other than they are to the third document's feature vector when using a metric such as 'Euclidean distance'. The 'Euclidean distance' between two vectors is equal to the Euclidean norm, or L2 norm, of the difference between the two vectors, as given by this equation:\n",
    "\n",
    "### d = ||x0 - x1||\n",
    "\n",
    "### A norm is a function that assigns a positive size to a vector. The Euclidean norm of a vector is equal to the vector's magnitude, which is given by the following equation:\n",
    "\n",
    "### ||x|| = sqroot of x1**2 + x2**2 + x3**2 xn**2\n",
    "\n",
    "\n",
    "### scikit-learn's 'euclidean_distance' function can be used to calculate the distance between two or more vectors, and confirms that the most semantically similar documents are also the closest to each other in the vector space. In the following example, we will use the 'euclidean_distance' function to compare the feature vectors for our documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between 1st and 2nd documents: [[2.44948974]]\n",
      "Distance between 1st and 3rd documents: [[2.64575131]]\n",
      "Distance between 2nd and 3rd documents: [[2.64575131]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "X = vectorizer.fit_transform(corpus).todense()\n",
    "print('Distance between 1st and 2nd documents:',\n",
    "     euclidean_distances(X[0], X[1]))\n",
    "print('Distance between 1st and 3rd documents:',\n",
    "     euclidean_distances(X[0], X[2]))\n",
    "print('Distance between 2nd and 3rd documents:',\n",
    "     euclidean_distances(X[1], X[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-dimensional vectors that have many zeroed elements are called 'SPARSE VECTORS'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using high-dimensional data creates several problems for all machine learning tasks, including those that do not involve text. Collectively, these problems are known as the CURSE OF DIMENSIONALITY. The first problem is that high-dimensional vectors require more memory and computation than low-dimensional vectors. SciPy provides some data types that mitigate this problem by efficiently representing only the non-zero elements of sparse vectors. The second problem is that as the feature space's dimensionality increases, more training data is required to ensure that there are enough training instances with each combination of the feature's values. \n",
    "\n",
    "### If there are insufficient training instances for a feature, the algorithm may overfit noise in the training data and fail to generalize. In the following sections, we will review several strategies for reducing the dimensionality of text features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOP WORD FILTERING\n",
    "\n",
    "### A basic strategy for reducing the dimensions of the feature space is to convert all of the text to lowercase. This is motivated by the insight that the letter case does not contribute to the meanings of most words; sandwich and Sandwich have the same meaning in most contexts. Capitalization may indicate that a word is beginning a sentence, but the bag-of-words model has already discarded all information from word order and grammar.\n",
    "\n",
    "\n",
    "### A second strategy is to remove words that are common to most of the documents in the corpus. These words, called STOP WORDS, frequently include determiners such as \"the\", \"a\", and \"an\"; auxiliary verbs such as \"do\", \"be\", and \"will\"; and prepositions such as \"on\", \"around\", and \"beneath\". Stop words are often functional words that contribute to the document's meaning through grammar rather than their denotations. 'CountVectorizer' can filter stop words provided as the 'stop_words' keyword parameter, and also includes a basic list of English stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 0 1 0 0 1]\n",
      " [0 1 1 1 0 1 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 1 0 0]]\n",
      "No Stop_Words {'unc': 9, 'played': 6, 'duke': 2, 'in': 4, 'basketball': 1, 'lost': 5, 'the': 8, 'game': 3, 'ate': 0, 'sandwich': 7}\n",
      "[[0 1 1 0 0 1 0 1]\n",
      " [0 1 1 1 1 0 0 0]\n",
      " [1 0 0 0 0 0 1 0]]\n",
      "With Stop_Words {'unc': 7, 'played': 5, 'duke': 2, 'basketball': 1, 'lost': 4, 'game': 3, 'ate': 0, 'sandwich': 6}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['UNC played Duke in basketball',\n",
    "         'Duke lost the basketball game',\n",
    "         'I ate a sandwich']\n",
    "\n",
    "# CountVectorizer with and without 'stop_words'\n",
    "\n",
    "vectorizer = CountVectorizer() # no stop_words\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print('No Stop_Words %s' % vectorizer.vocabulary_)\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words = 'english')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print('With Stop_Words %s' % vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "\n",
    "### While stop word filtering is an easy strategy for dimensionality reduction, most stop word lists contain only a few hundred words. A large corpus may still have hundreds of thousands of unique words after filtering. Two similar strategies for further reducing dimensionality are called stemming and lemmatization.\n",
    "\n",
    "### A high-dimensional document vector may separately encode several derived or inflected forms of the same word. For example, \"jumping\" and \"jumps\" are both forms of the word \"jump\"; a document vector in a corpus of long-jumping articles may encode each inflected form with a separate element in the feature vector. Stemming and lemmatization are two strategies for condensing inflected and derived forms of a word into a single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1]\n",
      " [0 1 1 0]]\n",
      "{'ate': 0, 'sandwiches': 3, 'sandwich': 2, 'eaten': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['He ate the sandwiches',\n",
    "         'Every sandwich was eaten by him']\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The documents have similar meanings, but their feature vectors have no elements in common! Both documents contain a conjugation of 'ate' and a form of 'sandwich'. Ideally, these similarities should be reflected in the feature vectors. \n",
    "\n",
    "\n",
    "### LEMMATIZATION is the process of determining the lemma, or the morphological root, of an inflected word based on its context. LEMMAS are the base forms of words that are used to key the word in a dictionary.\n",
    "\n",
    "### STEMMING has a similar goal to lemmatization, but it does not attempt to produce the morphological roots of words. Instead, stemming removes all patterns of characters that appear to be affixes, resulting in a token that is not necessarily a valid word. \n",
    "\n",
    "### Lemmatization frequently requires a lexical resource, like WordNet, and the word's part-of-speech.\n",
    "\n",
    "### Stemming algorithms frequently use rules instead of lexical resources to produce stems and can operate on any token, even without its context. Let's consider lemmatization of the word 'gathering' in two documents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the first sentence, 'gathering' is a verb, and its lemma is 'gather'.  In the second sentence, 'gathering' is a noun, and its lemma is 'gathering'. We will use the Natural Language Tool Kit (NLTK) to stem and lemmatize the corpus. \n",
    "\n",
    "### First I must install the NTLK package as shown in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in /Users/squeeko/Library/Python/3.7/lib/python/site-packages (3.4.5)\n",
      "Requirement already satisfied, skipping upgrade: six in /Users/squeeko/Library/Python/3.7/lib/python/site-packages (from nltk) (1.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/squeeko/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip3 install --user -U nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather\n",
      "gathering\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "corpus = ['I am gathering ingredients for the sandwich',\n",
    "         'There were many wizards at the gathering']\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('gathering', 'v')) # 'v' = verb\n",
    "print(lemmatizer.lemmatize('gathering', 'n')) # 'n' = noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I will compare lemmatization with stemming. 'PorterStemmer' cannot consider the inflected form's part-of-speech and returns 'gather' for both documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('gathering'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/squeeko/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/squeeko/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed:  [['He', 'ate', 'the', 'sandwich'], ['everi', 'sandwich', 'wa', 'eaten', 'by', 'him']]\n",
      "Lemmatized:  [['He', 'eat', 'the', 'sandwich'], ['Every', 'sandwich', 'be', 'eat', 'by', 'him']]\n"
     ]
    }
   ],
   "source": [
    "# Now I will lemmatize the toy corpus\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "wordnet_tags = ['n','v']\n",
    "corpus = [\n",
    "    'He ate the sandwiches',\n",
    "    'Every sandwich was eaten by him'\n",
    "]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "print('Stemmed: ', [[stemmer.stem(token) for token in \n",
    "                     word_tokenize(document)] for document in corpus])\n",
    "\n",
    "def lemmatize(token, tag):\n",
    "    if tag[0].lower() in ['n', 'v']:\n",
    "        return lemmatizer. lemmatize(token, tag[0].lower())\n",
    "    return token\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tagged_corpus = [pos_tag(word_tokenize(document)) for document in \n",
    "                 corpus]\n",
    "print('Lemmatized: ', [[lemmatize(token, tag) for token, tag in\n",
    "                       document] for document in tagged_corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending bag-of-words with tf-idf weights\n",
    "\n",
    "### In the previous section, we used the bag-of-words representation to create feature vectors that encode whether or not a word from the corpus's dictionary appears in a document.\n",
    "\n",
    "### These feature vectors do not encode grammar, word order, or the frequencies of words. It is intuitive that the frequency with which a word appears in a document could indicate the extent to which a document pertains to that word. A long document that contains one occurrence of a word may discuss an entirely different topic than a document that contains many occurrences of the same word. In this section, we will create feature vectors that encode the frequencies of words, and discuss strategies for mitigating two problems caused by encoding term frequencies. Instead of using a binary value for each element in the feature vector, we will now use an integer that represents the number of times that the words appeared in the document. With stop word filtering, the corpus is represented by the following feature vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 3 1 1]\n",
      "Token indices {'dog': 1, 'ate': 0, 'sandwich': 2, 'wizard': 4, 'transfigured': 3}\n",
      "The token \"dog\" appears 1 times\n",
      "The token \"ate\" appears 2 times\n",
      "The token \"sandwich\" appears 3 times\n",
      "The token \"wizard\" appears 1 times\n",
      "The token \"transfigured\" appears 1 times\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['The dog ate a sandwich, the wizard transfigured a sandwich, and I ate a sandwich']\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "frequencies = np.array(vectorizer.fit_transform(corpus).todense())[0]\n",
    "print(frequencies)\n",
    "print('Token indices %s' % vectorizer.vocabulary_) #indices are numbered based on alpha order\n",
    "for token, index in vectorizer.vocabulary_.items():\n",
    "     print('The token \"%s\" appears %s times' % (token, frequencies[index]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The element for 'dog' (at index 1) is now set to one, and the element for sandwich (at index 2) is set to three to indicate that the corresponding words occurred one and three times, respectively. Note that the 'CountVectorizer' binary parameter is omitted; its default value is 'False', which causes it to return raw term frequencies rather than binary frequencies.\n",
    "\n",
    "### Encoding the terms' raw frequencies in the feature vector provides additional information about the meanings of the documents, but assumes that all the documents are of similar lengths. Many words might appear with the same frequency in two documents, but the documents could still be dissimilar if one document is many times larger than the other.\n",
    "\n",
    "### scikit-learn's 'TfdfTransformer' can mitigate this problem by transforming a matrix of term frequency vectors into a matrix of normalized term frequency weights. By default, 'TfdfTransformer' smooths the raw counts and applies L2 normalization. The smoothed, normalized term frequencies are given by this equation:\n",
    "\n",
    "## tf(t,d) = f(t,d)/||x||\n",
    "\n",
    "### The numerator is the frequency of the term in the document. The denominator is the L2 norm of the term count vector. In addition to normalizing raw term counts, we can improve our feature vectors by calculating logarithmically scaled term frequencies, which scale the counts to a more limited range. Logarithmically scaled term frequencies are given by the following equation:\n",
    "\n",
    "## tf(t,d) = 1 + log f(t,d)\n",
    "\n",
    "### 'TfdfTransformer' calculates logarithmically scaled term frequencies when its 'sublinear_tf' keyword parameter is set to 'True'. Normalization and logarithmically scaled term frequencies can represent the frequencies of terms in a document while mitigating the effects of different document sizes. However, another problem remains with these representations. The feature vectors contain large weights for terms that occur frequently in a document, even if those terms occur frequently in most documents in the corpus. These terms do not help to represent the meaning of a particular document relative to the rest of the corpus. For example, most of the documents in a corpus of articles about Duke's basketball team could include the words 'Coach K', 'trip', and 'flop'. These words can be thought of as corpus-specific stop words, and may not be useful for calculating the similarity of documents. The Inverse Document Frequency (IDF) is a measure of how rare or common a word is in a corpus. The INVERSE DOCUMENT FREQUENCY is given by this equation:\n",
    "\n",
    "## idf(tD) = log(N/1+|d E D:t E d| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, the numerator is the total number of documents in the corpus and the denominator is the number of documents in the corpus that contain the term. A term's tf-idf value is the product of its term frequency and inverse document frequency. 'TfidTransformer' returns tf-idf weights when its 'use-idf' keyword argument is set to its default value,'True'.\n",
    "\n",
    "### Since tf-idf weighted feature vectors are commonly used to represent text; scikit-learn provides a'TfidVectorizer' transformer class that wraps 'CountVectorizer' and 'TfidTransformer' Let's use 'TfidVectorizer' to create tf-idf weighted feature vectors for our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.75458397 0.37729199 0.53689271 0.         0.        ]\n",
      " [0.         0.         0.44943642 0.6316672  0.6316672 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'The dog ate a sandwich and I ate a sandwich',\n",
    "    'The wizard transfigured a sandwich'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "print(vectorizer.fit_transform(corpus).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Will complete at a later date (hashes and word embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
